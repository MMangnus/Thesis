\section*{Methods}
Boundary Based Registration (BBR) is based on a cortical construction of the boundary between the white matter and grey matter, and the grey matter and the CSF (pial surface) \citep{Greve2009}. This surface is generated on an anatomical scan, as the contrast of the functional scan may not be good enough for segmentation. In order to register the two volumes, the constructed surface is moved to the functional image. The average contrast across the grey-white matter boundary is computed and used as a cost function to optimise the transformation parameters of the registration. Versions of BBR are implemented in FreeSurfer (\texttt{bbregister}) \citep{Dale1999} and FSL (\texttt{flirt -cost bbr}) \citep{Jenkinson2001}. This method is powerful enough to be able to register the volumes, based on only a small part of the brain \citep{Greve2009}, suggesting that local application can also be successfully employed. Especially for high resolution (laminar) fMRI, the anatomical and functional volumes contain detailed information about the gyrification that can be used for registration. We hence propose to extend BBR with a hierarchical strategy \citep{Collins1995}. By recursively applying BBR at diminishing scales as a series of linear transformation, the volumes are effectively non-linearly registered.

The way in which the mesh is divided is based on a three-dimensional cuboid lattice consisting of the set of neigbourhoods $\mathcal{N}^{d}$ at various depths $d$, decreasing in size. At each depth, $\mathcal{N}^{d}$ is divided into eight equally sized cuboids (two in each dimension) $\mathcal{N}^{d + 1}$ until a user specified threshold size is reached. All vertices that make up the brain mesh are divided over the elements of $\mathcal{V}^{d}$, the set of vertices within $\mathcal{N}^{d}$. The number of vertices in each neighbourhood may well vary between regions.

Registration is performed iteratively, from the largest scale ($d=0$) to the smallest ($d=d_{max}$). For each element $\mathcal{N}_{i}^{d}$, a registration is computed by means of a boundary based registration algorithm \citep{Greve2009} applied to $\mathcal{V}_{i}^{d}$. In short, this is an edge detection algorithm that maximises the average contrast across the white matter surface. Contrast is defined as the (optionally weighted) gradient of samples on either side of vertices within the mesh. For the optimisation procedure we use MATLAB's gradient descent method \texttt{fminsearch} (default parameters) to optimise registration parameters as a function of contrast. The same sampling method and cost function were used as proposed by Greve \& Fischl \citep{Greve2009}.

Applying the computed transformations directly to $\mathcal{V}^{d}$ could easily `break the mesh' at the edges of neighbouring regions as their continuity is not guaranteed. This is a serious problem, as subsequent steps in laminar analysis (like the level set methods \citep{Sethian1999}) require the mesh to be a topological sphere: a non-intersection closed surface without holes. In order to ensure continuity we use a control point based strategy \citep{Collins1995}: let the edges and corners of $\mathcal{N}^{d}$ define a deformable lattice. For each computed transformation in $\mathcal{N}^{d}$, a resulting displacement vector is assigned to all of its corner points. After all transformations at a depth level are computed, the median is taken of all displacement vectors for each control point, thus representing a resultant vector based on adjacent neighbourhoods. In order to further increase robustness (but at the cost of specificity), the displacement vectors may subsequently be adjusted based on their direct neighbours:
\begin{equation}
\vec{d}=\alpha \vec{d} + \left(1-\alpha\right) \mathbf{M}(\vec{x}),
\label{eq:alphasmoothing}
\end{equation}
where $\mathbf{M}(\vec{x})$ denotes the mean displacement of the neighbourhood. Collins et al. \citep{Collins1995} experimentally found $\alpha=0.5$ yields an acceptable balance between local matching and global smoothness. We recommend higher values for $\alpha$, as our primary goal in laminar analysis is the local matching. We suggest additional ways of increasing robustness in the next section.

As all control points within $\mathcal{N}^{d}$ now have displacement vectors associated with them, $\mathcal{V}^{d}$ can be displaced proportionally to the distance to their closest control points. A way of describing this is by defining transformation matrix $\mathbf{T}$, such that it satisfies:
\begin{equation}
\mathbf{D}_{i}^{d}=\mathbf{T}\mathcal{V}_{i}^{d},
\label{eq:regression}
\end{equation}
where the set of displacement vectors is denoted by $\mathbf{D}^{d}$, for which each $i$'th element contains 8 vectors, one for each corner of the cube. This is a typical linear regression equation and could hence trivially be solved for $\mathbf{T}$. However, a least squares solution is required as the system is overdetermined with eight corner points and only a $[4 X 4]$ transformation matrix. This necessarily requires an approximation of the deformation vectors that may result in discontinuities with respect to adjacent neighbourhoods. A preferred method is to divide the cube into six tetrahedra by means of Delaunay triangulation \citep{Delaunay1934} and compute a separate matrix for each of them. For each tetrahedron, equation~\ref{eq:regression} represents a determined system, for which the solution for adjacent tetrahedra is guaranteed to be continuous. Effectively, this division increases the degrees of freedom of the deformation field to satisfy our requirement of continuity. Having found the transformations, they can readily be applied to adjust the position of $\mathcal{V}^{d}$.

This procedure is repeated for all depths levels $d$. Whenever the number of vertices in $\mathcal{N}^{d}$ is smaller than a user specified minimum, the transformation for that region is set to the identity matrix. 

\subsection*{Robustness}
\label{sec:robustness}
With the exponential increase in number of neighbourhoods as a function of depth level, the number of registrations easily reaches into the hundreds. The high number of Degrees of Freedom (DoF) of the algorithm therefore inescapably increases the probability of misregistrations. The algorithm ensures robustness and continuity in several ways, by computing displacement vectors as a weighted average of all surrounding neighbourhoods, and by an (optional) additional smoothing of displacement vectors based on adjacent control points, as mentioned above. Additionally, we compute a transformation within a neighbourhood not only for the entire neighbourhood, but also for six subregions. By splitting $\mathcal{V}^{d}$ separately into two in the $x$, $y$, and $z$ dimensions, six cuboids are created for which the registration is repeated. This procedure is  illustrated in Fig.~\ref{fig:cubedivision}. The resulting displacement vectors are added to the respective list for each control point. 
\input{./Figures/CubeDivision}

\subsection*{Parameters}
The algorithm can look for any combination of translation, rotation and scaling in the $x$, $y$, and $z$ dimensions. It will divide the volume until it reaches a user defined minimum size or number of vertices, and a transformation will be computed. We here focus on registration in the phase enconding direction only, i.e. translation and scaling in the $y$-direction, as this is the most common type of distortion when an Echo Planar Imaging sequence is used \citep{Mansfield1977}. We set the minimum size of a neighbourhood to 4 voxels and the minimum number of vertices to 100. The smoothing factor with respect to adjacent vertices in the lattice from Eq.\ref{eq:alphasmoothing} was set to $\alpha=0.9$. Experimentally, we have found that this still yields robust results while still being highly weighted towards specificity. In contrast, Collins et al. \citep{Collins1995} describe an $\alpha$-level of $\alpha=0.5$, which is likely to do with the fact that their purpose of template matching and segmentation prioritises robustness over specificity. While the original implementation of BBR recommends using subsets of vertices for parts of the algorithm \citep{Greve2009}, we here use all vertices at all stages. This may be redundant for registrations at the large scale, but as it is a small addition in computation time, and because errors in early iterations may propagate further downwards, we chose to incorporate all vertices. Finding the best selection of parameters may still require some iterations for a given data set. It is, however, a substantial improvement with respect to the arduous job of manually matching small patches of cortex within a very limited field of view. 

\subsection*{Validation}
Assessing the quality of the registration performance proved challenging. While there is a clear theoretical relationship between the distortion size and the parameters of the acquisition \citep{Jezzard1995}, in practice it is difficult to find a gold standard for the submillimetre accuracy that we are aspiring to. Tools to convert a field maps to estimates of voxel displacement are usually heavily smoothed and cannot account for subject movement in the scanner or field changes between acquisitions. We hence created our own gold standard by acquiring an undistorted FLASH image and manually distorting it based on a field map. This way, the exact distortions were known in order to test if we could find them back with RBR. 

% % FLASH fieldmap
A high resolution whole brain multi-echo FLASH image \citep{Haase1986} was acquired at a 3T Siemens Scanner, TR=95 ms, $\alpha$=20\textdegree, bandwidth=170 Hz/px, [0.75 mm]$^3$. GRAPPA was used for three-fold in-plane acceleration. The echo times ranged from 5.88 ms to 78.96 ms with an echo spacing of 8.12 ms. The average of the last seven echoes was used, as the first three contained little contrast. This was accompanied by a whole brain MPRAGE acquisition that was used for the FreeSurfer cortical reconstruction, TR/TE/TI/$\alpha$ = 2300 ms/3.15 ms/1100 ms/8\textdegree, [0.8 mm]$^3$. A field map was acquired to realistically distort the FLASH image. The resolution was [3.5 mm X 3.5 mm X 2.0 mm], TR/TE1/TE2/$\alpha$ = 1020 ms/10 ms/12.46 ms/90\textdegree, bandwidth=260 Hz/Px.

The cortical reconstruction from FreeSurfer's \texttt{recon-all} \citep{Dale1999} was coregistered to the FLASH image using a 6 DoF linear registration with a custom MATLAB implementation of BBR. In order to distort the cortical surface, a voxel displacement map (VDM) was computed using SPM field map tools \citep{Andersson2001}. 
In order to taper unrealistically large displacements, and to move non-displaced vertices away from zero, the VDM was first transformed by a cubic root, after which it was applied in the anterior-posterior direction to the boundaries. Vertices were on average displaced by 2.56 mm (3.4 voxels), which is considerably more than could be tolerated for a laminar specific experiment. The distribution of displacement values was bimodally distributed away from zero with the specific goal to let RBR find the displacement and yield a sharp unimodal distribution around zero, as close to a delta distribution as possible.

% % EPI real
Additionally, RBR was tested on 12 brain scans obtained from a 7T scanner. We used 3D EPI \citep{Poser2010}, [0.93 mm]$^3$, TR/TE/$\alpha$ = 2768 ms/20 ms/14\textdegree, bandwidth=1167 Hz/pixel, phase encoding direction: A -> P, matrix size = 204$^2$, effective echo spacing = 0.25 ms. The boundaries were created by FreeSurfer on a whole-brain MP2RAGE (1.03 mm$^3$, TR/TE/TI1/TI2 = 5000 ms/1.89 ms/900 ms/3200 ms) \citep{Marques2010}. BBR was performed by \texttt{bbregister} with a full affine transformation (12 DoFs) on the mean of the functional images and subsequently, the boundaries were imported to MATLAB. Overlaying the registered boundaries on top of the EPI image showed clear local geometrical distortions in the phase encoding direction, related to field inhomogeneity \citep{Jezzard1995}. These distortions were of the order of several millimeters within a single volume. We corrected this with RBR and investigated its performance.

It must be noted that it is challenging to find an objective metric by which the quality of the registration could be quantified. The true distorted position is unknown and methods to approximate do not have the desired submillimetre specificity that is required. Moreover, if such metric existed, it could itself be used in the optimisation procedure. An alternative, however, is an investigation of RBR's performance on the true volume compared to the performance on the volume with different levels of added noise. Assuming that the algorithm has the best performance on the original data, the displacement with respect to the no-noise condition is expected to increase when more noise is added. Note that this does not make a statement about the correctness of the result. To investigate the RBR's performance in the presence of noise, we added twelve different levels of white noise to the data, applied RBR and compared the displacement to the unsalted version. For this, we used the Average Absolute Distance (AAD) \citep{Greve2009}, defined to be the average distance that the cortical surface moves between the two sets of registrations.
