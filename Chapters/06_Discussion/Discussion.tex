
\chapter{Summary and Discussion}
\chaptermark{Discussion}
\label{ch:discussion}

The layers of the cortex contain information about the nature of processes undertaken by brain regions and may reveal more information about inter-regional communication. Functional MRI is the most realistic method to date for studying the cortical layers in living human subjects, as it is spatially precise and non-invasive. The objective of this thesis was to pave the way for doing more robust and routine laminar fMRI analysis. The work presented in this thesis provides a significant step towards this end. We have developed several new methods that solve major problems in laminar analysis, we have implemented a full layer specific analysis, and we took explicit care to make the entire analysis pipeline reproducible and reusable.

First, we addressed the problem of local distortions that are often present in Echo Planar Images (EPI). Due to inhomogeneities in the main magnetic field, nuclei in some areas precess at different rates. Effectively, this causes small shifts in parts of the image with respect to the true position. Thus, even if the true locations of the layers are known in anatomical space, they will be displaced by distortions that can easily be larger than the thicknesses of the layers. Without correcting this effect, it is virtually impossible to retrieve any reliable layer signal. Chapter~\ref{ch:registration} introduced a method for addressing this type of distortion, Recursive Boundary Registration (RBR), which can achieve submillimetre accuracy.

Once the geometry of our cortical surfaces is properly aligned with our functional data, there is a further question that needs to be answered: how can the laminar signal be extracted from the MRI volume? There are several intuitive ways: a volume could be interpolated at the approximate location of the layers, or voxels could be classified to represent the most likely layer. However, both these methods inherently smears out the laminar signal to some extent. In Chapter~\ref{ch:glm} we set out to quantify this signal leakage and to present a new method designed to reduce it and to more cleanly separate the laminar signals by means of a spatial General Linear Model.

Having prepared the data for high-resolution accuracy with RBR, and having invented a way to extract laminar signals with the spatial GLM, we were ready to conduct an experimental study. In Chapter~\ref{ch:attention}, we investigated the laminar underpinning of visual attention. The visual system has been widely studied, and laminar level processing and anatomical organisation are relatively well known as a result of research in animals, which makes these regions well suited to be studied with laminar fMRI in humans. In general, in the primary visual cortex, visual input first arrives in the thalamus; from there, thalamo-cortical connections primarily target layer IV in the primary visual cortex. In contrast, layers I-II and VI typically receive downward information flow (feedback) from other regions in the cortex, for example when attending to specific features or parts of the incoming visual information. Therefore we investigated whether the BOLD signal in fact shows a laminar differentiation in feedback and feedforward signals.

But could someone else understand, reproduce, or replicate our study from Chapter~\ref{ch:attention}? Although the entire laminar analysis procedure can in principle be reconstructed from the methods section in the work described here, it would take a lot of time and effort to translate it into code for someone unfamiliar with the analysis. In addition, the methods describe the core of the analysis pipeline, but not the branches for data quality checks; sanity checks; or intermediate results for, for example, inspecting registration or layering quality. If the original code is provided, reproduction of the results should be much easier, but still not straightforward. It takes a lot of time to become accustomed to someone else's style of coding and to the programming languages and toolboxes that were used. Even if all these hurdles are overcome, it is still essential to have the appropriate software versions and licences to run the code. We addressed these problems by creating a tool called Porcupine to visually create, inspect, and share analyses, as shown in Chapter~\ref{ch:porcupine}.

\section*{Chapter~\ref{ch:registration}: Recursive Boundary Registration}
Geometrical transformations from one volume to the other (coregistration) have been very successful for volume-to-volume registration and are used routinely in fMRI analysis. Even when volumes have low contrast, low resolution, or few slices, they often contain enough information to compute simple (linear) transformations for an accurate coregistration \cite{Greve2009}. However, this may not be enough for more complex (non-linear) warpings. Non-linearities require a high number of degrees of freedom because of the many parameters that need to be estimated. This leaves more room for error and therefore requires high-contrast data sets (i.e., more information). Non-linear transformations are routinely used for transforming single subject anatomical space to a template space with the same strong grey-white matter contrast. However, they are not powerful enough to be applied for cross-contrast purposes, for example to undistort low-contrast EPI images. We have therefore developed a new technique for this: Recursive Boundary Registration (RBR). By recursively applying linear transformations at diminishing spatial scales, we effectively compute a non-linear registration. In order to guarantee smoothness over all transformations, it is combined with a control point lattice that regulates the transformations. We explicitly take the geometry of the brain into account, which provides a novel way to approach non-linear coregistration.

We tested RBR on two different types of data. First, in order to establish a gold standard with known distortions, we warped a FLASH image that was initially distortion-free. Because of the controlled warping, we could easily compare the performance of RBR to our ground truth and thereby verify the quality of the registration. Second, we examined a high-resolution EPI data set of 11 subjects that had real distortions. As the true size of the distortions was unknown, there could not be an absolute quality metric for the registration. Instead, by adding different levels of noise, we showed that there was a clear SNR dependence in the displacement that decreased towards the no-noise condition. Additionally, we provided an abundance of graphical evidence to illustrate the performance of RBR.

The power of the RBR approach lies in the fact that it makes explicit use of the cortical folding and the specific geometry of the individual brain. It is therefore more robust with lower contrast images and can still produce an accurate registration while preserving the topology of the original surfaces. Because of the large number of parameters that had to be estimated, we built in a number of robustness assurances. Despite the overall improved registration, it remained important to carefully inspect the quality of the registration to verify the required submillimetre accuracy. RBR proved to be a valuable tool for preparing Gradient Echo images for subsequent analyses; we therefore used it in our experimental study in Chapter~\ref{ch:attention}. It is now an integral part of the fMRI analysis toolbox for laminar fMRI: \url{https://github.com/TimVanMourik/OpenFmriAnalysis}.

A helpful perspective for understanding this coregistration method is a more conceptual view of the problem. The problem of coregistration can be classified along two main axes: the type of image contrast can be different or the same, and the required transformation can be linear or non-linear. The easiest scenario is to find a linear transformation for volumes with similar contrast. The problem becomes harder when the volumes have different contrasts, as the mapping of intensity values from one volume to the next is unknown. If instead the contrast is the same but the required transformation is non-linear, accurate solutions can still be found \cite{Collins1995}. However, when the contrast is different \emph{and} the registration should be non-linear, the degrees of freedom of the problem increases dramatically; so much so that the search space becomes too large to estimate. Combine this with the low contrast-to-noise ratio of fMRI data and it is clear that solving this problem is computationally infeasible with standard volume-to-volume registration techniques. The key to being able to do cross-contrast and non-linear registration is to introduce more prior knowledge into the equation. We here used the geometrical information of the cortex and its many gyri and sulci to more accurately estimate a non-linear cross-modal registration. This assumption only holds for volumes with the same geometry (within subject); so by introducing subject-specific prior knowledge, we restrict the number of potential applications of the method. Where the previous non-linear transforms could also compute subject-to-template registration, we lost this ability by strictly enforcing equal geometry across volumes. As a general notion, algorithms can become more powerful when they are more specialised. Usually this requires a specific type of prior knowledge in the data that is quantified and optimised.

\section*{Chapter~\ref{ch:glm}: Spatial GLM for laminar FMRI}
In order to extract the laminar signal from the cortex, we started out with the notion that all voxels contain contributions from a number of layers. If this mixture could be accurately modelled, it could theoretically also be inverted. Inverting a model with a General Linear Model (GLM) with voxel specific layer mixtures could yield the layer intensity values. To achieve this, we first set up a mathematical framework to accurately model the layer distribution and subsequently estimate the layer signal intensity. We incorporated information about the precise location, curvature, and thickness of the cortex into the modelling perspective. For the estimation procedure we suggested taking into account the noise correlation of the volume. Interestingly, we found a way to describe existing procedures in the same mathematical framework of the laminar spatial GLM. This allowed for easy comparison between procedures in the subsequent validation.

In validating the performance of all methods, we used several different types of data. In order to see if in principle the method would perform better when all assumptions were met, we made a highly accurate simulation of the cortex. By modelling known properties about the curvature into the gyri and sulci of the cortex, we constructed a gold standard of an MRI volume. This allowed for a maximally clean comparison between the different methods in ideal circumstances. As predicted, extracting the laminar signal by means of a spatial GLM considerably reduced the signal leakage to neighbouring layers. We then investigated the performance of all three methods with two types of 
emph{in vivo} data: a high-resolution (post-mortem) part of the cortex that allows for the specification of a large number of cortical layers, and a set of \emph{in vivo} human structural scans to gauge the viability of the methods in live human MRI data. We found largely similar behaviours between methods, but also a disturbing artefact in the spatial GLM profiles: an oscillating pattern that became stronger mainly when the cortex was divided into more layers. The performance and the strength of the ringing artefact also depend on a vast parameter set that would be interesting to investigate. These parameters include, amongst others, the volume's spatial resolution, noise level, noise type, the presence or absence of veins, and intensity gradients. Investigating the combination of all these factors would have created a combinatorial explosion, so we carefully chose a subset and presented the comparative results.

Thus, we provide a more formal mathematical description of layer separation. Despite the mathematical rigour, we find that the solution is more prone to error. We have previously mentioned that methods can become more powerful when they use types of prior knowledge inherent to the data. As an important addition, this study shows that this power might come at the cost of a high susceptibility to cases where the assumptions are violated. The fact that artefacts appear in the profile for the spatial GLM may well be interpreted as inaccurate boundary placement or the presence of unknown types of noise. While it is the GLM method that amplifies these inaccuracies, it should be noted that they are also present when other techniques for layer extraction methods are used. This may be a serious concern for laminar analysis in general, and still requires thorough examination. The presence of these artefacts again stresses the importance of carefully inspecting one's data before proceeding to making laminar inferences. We find that segmentations with one layer per voxel over the thickness of the cortex still yield stable solutions. We thus suggest that this should be the guideline for laminar resolution and that a higher number of layers may provide a false sense of spatial specificity.

\section*{Chapter~\ref{ch:attention}: Layer Specificity in Visual Attention}
In Chapter~\ref{ch:attention} we describe an experiment aimed at dissociating visual and attentional inputs to the primary visual cortex at the laminar level. We constructed an experiment in which we balanced visual input and attentional input in an orientation discrimination task. This experiment was carried out at high field, 7 Tesla, for higher sensitivity and submillimetre resolution. We took explicit care to remove as many sources of noise as possible: we corrected the distortions in the volume with RBR; filtered out heartbeat and respiratory noise with RETROICOR; established regions of interest from a dedicated retinotopic session; had a high number of subjects, a powerful statistical design, and performed more rigorous statistical tests than ever conducted in a laminar study; looked at three regions of interest of the visual hierarchy (V1, V2, V3); and performed numerous control analyses. Despite all this, we found no evidence for layer specific differentiation in the BOLD signal.

While we did not find statistical differences at the layer level, at the level of the region of interest we did find activation for feedforward and feedback stimuli. In line with previous findings, feedback (attentional) activation increases towards higher visual areas, whereas feedforward (visual) activation decreases \cite{Murray2008,Jehee2011}. Additionally, there is a steady increase in activation towards the superficial layers. This has frequently been observed and has two main potential explanations \cite{Koopmans2010,Polimeni2010}. The blood flows from deep layers to superficial layers and may cause a BOLD carry-over effect; alternatively, the pial surface has large draining veins to which gradient echo is sensitive, which may leak signal into lower layers as a methodological artefact. The lack of layer specific differentiation is difficult to interpret. When an experiment yields a null result, there are a variety of potential causes that cannot easily be distinguished. They can be separated into three categories: methodological, neurophysiological, and statistical.

As mentioned throughout this thesis, exceedingly precise measurements are required for structures as small as the cortical layers. During the data analysis, many methodological challenges were resolved, but we cannot exclude the possibility of residual structural misalignments, for example due to cortical reconstruction or registration inaccuracies, dynamic misalignments due to movement, or cardiac pulsation of the brain. Additionally, we made several compromises because of the limits of the acquisition. The price we paid for a high spatial resolution and sensitivity was a low SNR per voxel and reduced specificity as a result of draining veins on top of the cortex. Other choices might have revealed more signal or reduced the amount of noise. Methodological issues that are not sufficiently addressed may in the worst case lead to a bias in the results, and even in the best case they may increase the noise surrounding the effect. If there is a true effect, it will only count as statistically significant if the average effect is stronger than the sources of noise that are inherently present in any measurement. We reduced the sources of noise and tried to increase the signal by scanning a larger number of participants than is customary for laminar studies: N=17 (cf. N=6 in \cite{Polimeni2010}, N=4 in \cite{Muckli2015}, N=10 in \cite{Kok2016}). Nonetheless, the signal term may still be small. Neurophysiologically, it is uncertain if targeted layers have a large effect on the laminar BOLD signal. While feedforward and feedback signals preferentially target specific layers, the neurophysiological signal may spread nearly instantaneously to other layers. On top of that, the neurovascular coupling does not guarantee that this is uniquely visible in the respective layer. An alternative explanation could be that our attentional modulation is fundamentally different from other cognitive capacities that have shown positive laminar differentiation, such as figure-ground segregation \cite{Kok2016} or other non-classical receptive field effects \cite{Muckli2015}.

In general, it is hard to specify the reason behind the null result, but it is interesting to put it in a broader perspective and compare it to other positive results in the literature. Given the difficulty of the analysis, the small effects, and the large number of sources of noise, one might expect many publications with laminar null results. However, to date, no paper exists that has specifically hypothesised and looked for laminar differentiation but found none, suggesting a substantial publication bias in this research field. There are understandable reasons for this. A negative result might be due to one or more of many possible errors made during the experiment. Identifying what underlies a negative result, and whether it is a true negative or related to unknown errors, is very hard. A positive result, on the other hand, is more readily accepted as not being due to error but to a true experimental effect, and will therefore find an easier path to publication. However, this will undoubtedly keep papers of similar quality in the file drawer, primarily because of their outcomes. Therefore, the state of the literature does not necessarily provide an accurate reflection of all scientific work \cite{Ioannidis2005,Button2013}. A critical reflection on this publication bias would be a welcome addition to the literature.

\section*{Chapter~\ref{ch:porcupine}: Pipelines for FMRI with Porcupine}
By visually creating a pipeline, the conceptual understanding is prioritised over the implementational details. This allows researchers to think more creatively about solving problems and gives them a better overview of their methods. At the same time it addresses many problems at the core of the reproducibility crisis in neuroimaging \cite{Nature2017,Munafo2017}. As Porcupine creates analysis code, one can now easily complement the methods section with a visual representation of the pipeline, which automatically translates to a workable script. The graphical pipeline is easily modifiable by non-coders, but the produced script could also be used by expert coders as a template to continue scripting. Additionally, we created a minimal operating system as a Docker file. This ensures that identical software versions are used and thus creates a completely reproducible environment.

In fMRI analysis, it is customary to perform many sequential steps on the data to make it maximally sensitive to the relevant dimensions in the data, and to reduce it to a summary statistic of neurocognitive interest. This type of data analysis is by no means restricted to fMRI analysis alone. It is present in other types of neuroimaging (e.g., EEG/MEG analysis, \cite{Oostenveld2011}), bioinformatics \cite{Wolstencroft2013}, and also in non-scientific areas such as computer graphics \cite{Blender}. Visual programming tools of Porcupine's kind are also not unique; not even in MRI analysis \cite{Lucas2010}. It is therefore interesting to consider why they have not been adopted by the community. Part of the reason may be that scientists often try to solve very specific questions, and do not necessarily realise the broad scope of applications for which such tools could be used, or complementary tools to which they could be connected. As a result, tools that could be of generic use may instead be written for personal use and thus not necessarily be compatible or consistent with similar software in the field. While common in almost every branch of software engineering, major fMRI analysis toolboxes have dedicated little effort to specifying communication between software components in an application programming interface (API). By and large, the characteristics of the software are a reflection of an organisational structure with high degrees of specialisation into independent tasks, and of a culture where individual accomplishments are more valued than the accomplishments of a team. From the user and developer perspectives, the unfortunate consequences are difficulties in getting familiar with and maintaining a package when it accumulates knowledge, methods, and new insights. Additionally, there is less than fertile ground for efforts to streamline the analysis, such as visual programming applications. The only neuroimaging package that makes an explicit attempt at consistency throughout its interface is Nipype, so this is the package to which we linked Porcupine. Nipype has the added benefit of linking to major MRI analysis packages in the community, so that it is the most diverse MRI analysis tool to date.

FMRI analysis pipelines are increasing in complexity, and the era of simple blobological studies is over. Logically, then, the learning curve to get to state-of-the-art methods and cutting-edge science is longer than in the early days of fMRI. If we keep reinventing old wheels by rewriting long analysis scripts or reimplementing software in many packages, this may inadvertently slow down the scientific process of new discoveries. Science will need to find a way to keep these longer pipelines insightful and keep the analysis manageable. With Porcupine we have made a considerable step towards this end. Additionally, Porcupine was fully made in the spirit of open science: the software is open source for everybody to contribute. Throughout development, scalability of the software has been a priority. Porcupine can easily be extended to other packages within or outside neuroimaging; its modular components may also be extended with running pipelines or visualising data. We believe that this is a valuable direction for a more open, reproducible, and sustainable science.

\section{General Discussion}
In this thesis, we have presented a variety of new tools for laminar analysis, made them available in an open source toolbox, linked it to our new graphical pipeline interface, and conducted a laminar fMRI experiment. This PhD project had as its main goal to make laminar analysis a more routine process and to learn more about directional communication in the cortex. We can safely say that we succeeded in streamlining this process, although we did not find laminar differentiation in our experimental study. Our initial hypothesis was that the layer specific BOLD signal would reflect the initial target layers for neuronal input. We have to consider that this picture is too simplistic to capture the complex operations that are being performed by the brain. It is important to further investigate how neuronal input to the cortex propagates to other layers, and how this is subsequently reflected in the laminar quantities such as BOLD, but also CBV, CBF, and CMRO$_2$. We have provided the tools to perform laminar analysis, independent of contrast mechanism, and now the main outstanding question is: can we use it to investigate the true nature of the laminar signal?

Recent work has shown higher layer specific accuracy for CBV than for BOLD in the motor cortex \cite{Huber2018}. Huber's BOLD/CBV study is a good example of the relevance of comparative studies and could be helpful in achieving a better understanding of the neurovascular coupling and neurophysiological processes. It is also interesting to see that the authors report results with 20 intermediate layers and perform control analyses with the spatial GLM. They comment that the spatial GLM method comes with significant noise enhancement, but it may be worth considering that the signal to distinguish 20 layers was never there in the first place. This limitation of the data is not revealed in classical (oversampled) interpolation, but may easily give a false sense of accuracy with respect to the spatial specificity.

In this thesis, we encountered numerous potential sources of noise. We took care to remove as many of them as possible, but not everything could be filtered out. A logical conclusion for future studies would be to increase the signal, for example by investing in even higher field strengths. Unfortunately, this may be subject to the law of diminishing returns. While initially the SNR was thought to increase almost quadratically (a power of 7/4 \cite{Hoult1979}), this was later shown to be only mildly superlinearly \cite{Hoult2000}. And with a linear decrease of the voxel length, its size (and SNR) decreases cubically, so the resolution may not improve much with higher field strength. Additionally, higher fields typically show higher field inhomogeneities and consequently higher distortions that cannot be corrected with RBR anymore. It will also put higher pressure on hardware and pulse sequences, since the T$_2^*$-values for tissue, blood and deoxyhaemoglobin decrease as a function of field strength. Thus, many technical challenges still need to be overcome before higher fields can adequately aid in the laminar investigation.

Could new developments in the analysis technique resolve these difficulties? The placement of layers is still a point that requires major improvement. Where we proposed image-based distortion correction with RBR, other methods try to use acquisition time strategies with topup \cite{Smith2004}, or acquire structural images with the same distortions \cite{Kashyap2017}. These methods all have their upsides and downsides and still stand to be improved. The subsequent step of dealing with the partial volume effect with the spatial GLM also leaves significant room for improvement. We laid out an initial strategy of creating a layer model and unmixing layer specific signal, but the results still show methodological artefacts and noise amplification. We identified a number of underlying sources, but the analysis stands to benefit from a more thorough investigation and quantification of the factors that contribute to this problem. 

Apart from better methods and a better understanding of the neurovascular coupling, it is equally important to better understand cognitive processes at the laminar level via other routes. The ultimate goal of laminar analysis is to provide answers to these questions, but as we are still in the early days of laminar fMRI, it goes both ways: in order to validate laminar analysis as a method it is important to have benchmark scenarios against which it can be tested. And at some point, science does not only need more big data but big theories \cite{Jensen2014}. More focus on the conceptual and computational nature of the cortical layers is needed to generate testable hypotheses. Only the combination of viable theories and experimental data will give us a better understanding of the human brain and mind. 




